{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d2a408-d06d-4d67-95c0-72cfd86dbbad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "library(quanteda)\n",
    "library(quanteda.textstats)\n",
    "library(writexl)\n",
    "library(seededlda)\n",
    "library(dplyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00be7e80-900b-4694-9d28-8ce945939c86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data and list column names\n",
    "convos <- read.csv('data/all-conversations.csv')\n",
    "\n",
    "# Focus our analysis on only the turns purely in the MAIN section - note that there are a small number of turns that overlap \n",
    "# between main and pre/post as different conversation groups were overlapping at the time of data collection.\n",
    "convos <- convos[convos$section == 'MAIN',]\n",
    "\n",
    "# generate a doc_id for quanteda - this would ideally be a primary key instead.\n",
    "convos$doc_id <- seq_along(convos$text)\n",
    "\n",
    "nrow(convos)\n",
    "names(convos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d220c59-2e47-4333-8d9f-a37c7343c485",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a corpus from the dataframe we loaded, use the text field as the text\n",
    "convo_corpus <- corpus(convos, text_field=\"text\", docid_field = \"doc_id\")\n",
    "\n",
    "# Tokenise the full dataset - use standard tokeniser, but remove punctuation and lowercase all tokens\n",
    "convo_full_tokens <- tokens_tolower(\n",
    "    tokens(convo_corpus, remove_punct=TRUE)\n",
    ")\n",
    "\n",
    "print(paste(\"Total tokens\", sum(ntoken(dfm(convo_full_tokens)))))\n",
    "print(paste(\"Total types\", length(types(convo_full_tokens))))\n",
    "\n",
    "frequent_tokens <- c(\n",
    "    # This is based on the top 100 most frequent tokens after\n",
    "    # removing the standard English stop list, preserving some common\n",
    "    # nouns like \"Brisbane\", \"Australia\", and \"friends\"\n",
    "    \"yeah\",\n",
    "    \"like\",\n",
    "    \"hh\",\n",
    "    \"um\",\n",
    "    \"know\",\n",
    "    \"just\",\n",
    "    \"oh\",\n",
    "    \"mm\",\n",
    "    \"really\",\n",
    "    \"haha\",\n",
    "    \"think\",\n",
    "    \"well\",\n",
    "    \"uh\",\n",
    "    \"cause\",\n",
    "    \"go\",\n",
    "    \"ok\",\n",
    "    \"get\",\n",
    "    \"people\",\n",
    "    \"one\",\n",
    "    \"right\",\n",
    "    \"hahaha\",\n",
    "    \"tsk\",\n",
    "    \"ah\",\n",
    "    \"got\",\n",
    "    \"good\",\n",
    "    \"hmm\",\n",
    "    \"kind\",\n",
    "    \"mean\",\n",
    "    \"can\",\n",
    "    \"kinda\",\n",
    "    \"lot\",\n",
    "    \"actually\",\n",
    "    \"okay\",\n",
    "    \"pretty\",\n",
    "    \"different\",\n",
    "    \"cool\",\n",
    "    \"little\",\n",
    "    \"much\",\n",
    "    \"thing\",\n",
    "    \"now\",\n",
    "    \"year\",\n",
    "    \"stuff\",\n",
    "    \"something\",\n",
    "    \"yep\",\n",
    "    \"went\",\n",
    "    \"back\",\n",
    "    \"years\",\n",
    "    \"two\",\n",
    "    \"bit\",\n",
    "    \"sort\",\n",
    "    \"time\",\n",
    "    \"see\",\n",
    "    \"even\",\n",
    "    \"say\",\n",
    "    \"going\",\n",
    "    \"nice\",\n",
    "    \"things\",\n",
    "    \"come\",\n",
    "    \"gonna\",\n",
    "    \"wanna\",\n",
    "    \"us\",\n",
    "    \"interesting\",\n",
    "    \"guess\",\n",
    "    \"work\",\n",
    "    \"states\",\n",
    "    \"still\",\n",
    "    \"probably\",\n",
    "    \"big\",\n",
    "    \"new\",\n",
    "    \"yes\",\n",
    "    \"way\",\n",
    "    \"first\",\n",
    "    \"never\",\n",
    "    \"three\",\n",
    "    \"sure\",\n",
    "    \"hhh\",\n",
    "    \"around\",\n",
    "    \"always\",\n",
    "    \"everything\",\n",
    "    \"i-\",\n",
    "    \"said\",\n",
    "    \"wow\",\n",
    "    \"came\",\n",
    "    \"ha\",\n",
    "    \"mhm\",\n",
    "    \"alright\",\n",
    "    \"hahahaha\",\n",
    "    \"day\",\n",
    "    \"take\",\n",
    "    \"want\",\n",
    "    \"used\",\n",
    "    \"long\",\n",
    "    \"y-\"\n",
    ")\n",
    "transcription_marks <- c(\n",
    "    \"Â°\",\n",
    "    \"sniffs\",\n",
    "    \"hh\",\n",
    "    \"hhh\"\n",
    ")\n",
    "custom_stopwords <- c(\n",
    "    frequent_tokens,\n",
    "    transcription_marks,\n",
    "    stopwords(\"english\")\n",
    ")\n",
    "\n",
    "# Tokenise the corpus - we use the Quanteda default tokenisation and remove the standard list of English stopwords\n",
    "# Note that the standard English list assumes written not spoken material so we will have to take a closer look at this.\n",
    "convo_tokens <- convo_full_tokens |> \n",
    "# Remove our custom stopwords\n",
    "tokens_remove(custom_stopwords, min_nchar=2) |>\n",
    "# Remove partial tokens using a match on trailing hyphen\n",
    "tokens_remove(\"*-\", valuetype=\"glob\")\n",
    "\n",
    "# Create a document-feature matrix, (also known as document term, or term-document matrix).\n",
    "# Note that dfm is the quanteda standard nomenclature so I'll use it throughout.\n",
    "# More specifically this is a Turn-Token matrix, as the 'documents' are single turns by a speaker.\n",
    "# For computational reasons we will trim any token that occurs only once to reduce the vocabulary size.\n",
    "convo_turn_dfm <- dfm_trim(dfm(convo_tokens), min_termfreq = 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd56ed9-4e24-4928-8550-9056c0ba88a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Granularity\n",
    "\n",
    "We can count things at different levels of granularity to start to address topic. A word that is used once in every conversation is different from a word used 30 times in a single conversation.\n",
    "\n",
    "Some examples:\n",
    "\n",
    "- we can count tokens, regardless of where they occur\n",
    "- we can count turns including a token\n",
    "- we can count conversations including a token\n",
    "- we can count by speaker in a conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852afca2-80e1-466d-a558-a56df8f8f95c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's start with tokens and turns:\n",
    "frequencies <- textstat_frequency(convo_turn_dfm)\n",
    "names(frequencies)\n",
    "\n",
    "# the feature is the token, the frequency is the token count, and docfreq is the turn count\n",
    "write_xlsx(frequencies, \"results/token_turn_counts.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878ac6cb-487d-44e3-af9f-d2b71f370d44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We'll group the dfm together by the file (conversation) to count tokens and conversations\n",
    "convo_dfm <- dfm_group(convo_turn_dfm, groups=docvars(convo_corpus, 'X_file_id'))\n",
    "conversation_frequencies <- textstat_frequency(convo_dfm)\n",
    "\n",
    "write_xlsx(conversation_frequencies, \"results/token_conversation_counts.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a452475a-32c2-4434-bdbe-cdf2978cca16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Topics by turn\n",
    "set.seed(192038102)\n",
    "lda_turns <- textmodel_lda(convo_turn_dfm, k=20)\n",
    "turn_results <- terms(lda_turns, n=20) |> as.data.frame()\n",
    "\n",
    "write_xlsx(turn_results, \"results/turn_topic_top_words.xlsx\")\n",
    "\n",
    "turn_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1592522a-974d-4d14-b77f-543fc83924e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Topics on conversations as documents\n",
    "set.seed(50193853)\n",
    "lda_convo <- textmodel_lda(convo_dfm, k=20)\n",
    "convo_results <- terms(lda_convo, n=20) |> as.data.frame()\n",
    "\n",
    "write_xlsx(convo_results, \"results/convo_topic_top_words.xlsx\")\n",
    "\n",
    "convo_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a52738f-37e2-42b5-8542-17b7b0e5f5c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "options(repr.matrix.max.rows=500)\n",
    "        \n",
    "# Keyword in (conversational) context - to do this we need to retrieve the turns containing the keyword, \n",
    "# and show it in context of surrounding turns.\n",
    "\n",
    "# Show this number of turns either side of each match\n",
    "turn_window_size = 3\n",
    "pattern = 'rock'\n",
    "\n",
    "# This is annoyingly fiddly - grab the matching turns and extract the original doc_id/sequence in our corpus df.\n",
    "# I feel like there should be a better way to do this?\n",
    "# We also need to take the unique docids and pre-compute window offsets for displaying the surrounding context. \n",
    "matching_turns <- data.frame(matchid = as.integer(unique(index(convo_tokens, pattern = pattern)$docname))) |>\n",
    "    mutate(window_start = matchid - turn_window_size, window_end = matchid + turn_window_size) |>\n",
    "    select(matchid, window_start, window_end)\n",
    "\n",
    "turns_with_context <- convos |> inner_join(matching_turns, join_by(between(doc_id, window_start, window_end))) |>\n",
    "    arrange(matchid, doc_id) |>\n",
    "    select(name, text, timeCode, X_file_id, matchid, doc_id)  \n",
    "\n",
    "filename <- paste(\"results/snippets_\", pattern, \".xlsx\", sep = \"\", collapse = NULL)\n",
    "\n",
    "print(paste(\"wrote out \", nrow(matching_turns), \" matches to: \", filename, sep=\"\", collapse=NULL))\n",
    "\n",
    "write_xlsx(turns_with_context, filename)\n",
    "\n",
    "turns_with_context\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
