{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45d2a408-d06d-4d67-95c0-72cfd86dbbad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Package version: 3.3.1\n",
      "Unicode version: 15.1\n",
      "ICU version: 75.1\n",
      "\n",
      "Parallel computing: 16 of 16 threads used.\n",
      "\n",
      "See https://quanteda.io for tutorials and examples.\n",
      "\n",
      "Loading required package: proxyC\n",
      "\n",
      "\n",
      "Attaching package: ‘proxyC’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:stats’:\n",
      "\n",
      "    dist\n",
      "\n",
      "\n",
      "\n",
      "Attaching package: ‘seededlda’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:stats’:\n",
      "\n",
      "    terms\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(quanteda)\n",
    "library(quanteda.textstats)\n",
    "library(writexl)\n",
    "library(seededlda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00be7e80-900b-4694-9d28-8ce945939c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'speakerID'</li><li>'text'</li><li>'timeCode'</li><li>'.file_id'</li><li>'name'</li><li>'X.id'</li><li>'X.type'</li><li>'nationality'</li><li>'doc_id'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'speakerID'\n",
       "\\item 'text'\n",
       "\\item 'timeCode'\n",
       "\\item '.file\\_id'\n",
       "\\item 'name'\n",
       "\\item 'X.id'\n",
       "\\item 'X.type'\n",
       "\\item 'nationality'\n",
       "\\item 'doc\\_id'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'speakerID'\n",
       "2. 'text'\n",
       "3. 'timeCode'\n",
       "4. '.file_id'\n",
       "5. 'name'\n",
       "6. 'X.id'\n",
       "7. 'X.type'\n",
       "8. 'nationality'\n",
       "9. 'doc_id'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"speakerID\"   \"text\"        \"timeCode\"    \".file_id\"    \"name\"       \n",
       "[6] \"X.id\"        \"X.type\"      \"nationality\" \"doc_id\"     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data and list column names\n",
    "convos <- read.csv('data/all-conversations-sample.csv')\n",
    "\n",
    "# generate a doc_id for quanteda - this would ideally be a primary key instead.\n",
    "convos$doc_id <- seq_along(convos$text)\n",
    "\n",
    "names(convos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d220c59-2e47-4333-8d9f-a37c7343c485",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a corpus from the dataframe we loaded, use the text field as the text\n",
    "convo_corpus <- corpus(convos, text_field=\"text\", docid_field = \"doc_id\")\n",
    "\n",
    "# Tokenise the corpus - we use the Quanteda default tokenisation and remove the standard list of English stopwords\n",
    "# Note that the standard English list assumes written not spoken material so we will have to take a closer look at this.\n",
    "convo_tokens <- tokens(\n",
    "    convo_corpus, remove_punct=TRUE\n",
    ") |> tokens_remove(stopwords(\"english\"))\n",
    "\n",
    "# Create a document-feature matrix, (also known as document term, or term-document matrix).\n",
    "# Note that dfm is the quanteda standard nomenclature so I'll use it throughout.\n",
    "# More specifically this is a Turn-Token matrix, as the 'documents' are single turns by a speaker.\n",
    "convo_turn_dfm <- dfm(convo_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd56ed9-4e24-4928-8550-9056c0ba88a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Granularity\n",
    "\n",
    "We can count things at different levels of granularity to start to address topic. A word that is used once in every conversation is different from a word used 30 times in a single conversation.\n",
    "\n",
    "Some examples:\n",
    "\n",
    "- we can count tokens, regardless of where they occur\n",
    "- we can count turns including a token\n",
    "- we can count conversations including a token\n",
    "- we can count by speaker in a conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "852afca2-80e1-466d-a558-a56df8f8f95c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'feature'</li><li>'frequency'</li><li>'rank'</li><li>'docfreq'</li><li>'group'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'feature'\n",
       "\\item 'frequency'\n",
       "\\item 'rank'\n",
       "\\item 'docfreq'\n",
       "\\item 'group'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'feature'\n",
       "2. 'frequency'\n",
       "3. 'rank'\n",
       "4. 'docfreq'\n",
       "5. 'group'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"feature\"   \"frequency\" \"rank\"      \"docfreq\"   \"group\"    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's start with tokens and turns:\n",
    "frequencies <- textstat_frequency(convo_turn_dfm)\n",
    "names(frequencies)\n",
    "\n",
    "# the feature is the token, the frequency is the token count, and docfreq is the turn count\n",
    "write_xlsx(frequencies, \"results/token_turn_counts.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "878ac6cb-487d-44e3-af9f-d2b71f370d44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We'll group the dfm together by the file (conversation) to count tokens and conversations\n",
    "convo_dfm <- dfm_group(convo_turn_dfm, groups=docvars(convo_corpus, '.file_id'))\n",
    "conversation_frequencies <- textstat_frequency(convo_dfm)\n",
    "\n",
    "write_xlsx(conversation_frequencies, \"results/token_conversation_counts.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a452475a-32c2-4434-bdbe-cdf2978cca16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 20 × 10</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>topic1</th><th scope=col>topic2</th><th scope=col>topic3</th><th scope=col>topic4</th><th scope=col>topic5</th><th scope=col>topic6</th><th scope=col>topic7</th><th scope=col>topic8</th><th scope=col>topic9</th><th scope=col>topic10</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>like       </td><td>oh         </td><td>um      </td><td>know    </td><td>like     </td><td>uh       </td><td>yeah     </td><td>yeah      </td><td>like    </td><td>really      </td></tr>\n",
       "\t<tr><td>just       </td><td>ok         </td><td>one     </td><td>well    </td><td>hh       </td><td>know     </td><td>mm       </td><td>haha      </td><td>just    </td><td>good        </td></tr>\n",
       "\t<tr><td>know       </td><td>okay       </td><td>like    </td><td>go      </td><td>um       </td><td>got      </td><td>hmm      </td><td>hahah     </td><td>hh      </td><td>little      </td></tr>\n",
       "\t<tr><td>kind       </td><td>yeah       </td><td>school  </td><td>ah      </td><td>different</td><td>come     </td><td>nice     </td><td>think     </td><td>really  </td><td>think       </td></tr>\n",
       "\t<tr><td>people     </td><td>haha       </td><td>year    </td><td>right   </td><td>kinda    </td><td>australia</td><td>haha     </td><td>hahaah    </td><td>hahaha  </td><td>bit         </td></tr>\n",
       "\t<tr><td>say        </td><td>yep        </td><td>actually</td><td>now     </td><td>cause    </td><td>always   </td><td>meet     </td><td>definitely</td><td>go      </td><td>um          </td></tr>\n",
       "\t<tr><td>things     </td><td>hahaha     </td><td>cause   </td><td>see     </td><td>lot      </td><td>work     </td><td>hoping   </td><td>right     </td><td>wanna   </td><td>can         </td></tr>\n",
       "\t<tr><td>get        </td><td>yes        </td><td>get     </td><td>mean    </td><td>sort     </td><td>never    </td><td>hm       </td><td>hi        </td><td>much    </td><td>still       </td></tr>\n",
       "\t<tr><td>stuff      </td><td>really     </td><td>time    </td><td>just    </td><td>whole    </td><td>something</td><td>hahaha   </td><td>true      </td><td>gonna   </td><td>like        </td></tr>\n",
       "\t<tr><td>think      </td><td>good       </td><td>two     </td><td>going   </td><td>well     </td><td>get      </td><td>times    </td><td>live      </td><td>thought </td><td>moved       </td></tr>\n",
       "\t<tr><td>everything </td><td>cool       </td><td>friends </td><td>went    </td><td>think    </td><td>small    </td><td>looks    </td><td>long      </td><td>want    </td><td>pretty      </td></tr>\n",
       "\t<tr><td>cause      </td><td>god        </td><td>went    </td><td>states  </td><td>english  </td><td>states   </td><td>alright  </td><td>nothing   </td><td>day     </td><td>people      </td></tr>\n",
       "\t<tr><td>interesting</td><td>hh         </td><td>degree  </td><td>new     </td><td>even     </td><td>years    </td><td>edinburgh</td><td>days      </td><td>stuff   </td><td>s-          </td></tr>\n",
       "\t<tr><td>talk       </td><td>wow        </td><td>first   </td><td>probably</td><td>guess    </td><td>worked   </td><td>flights  </td><td>every     </td><td>one     </td><td>conversation</td></tr>\n",
       "\t<tr><td>everyone   </td><td>take       </td><td>back    </td><td>back    </td><td>got      </td><td>maybe    </td><td>fruit    </td><td>spent     </td><td>going   </td><td>find        </td></tr>\n",
       "\t<tr><td>way        </td><td>food       </td><td>couple  </td><td>used    </td><td>america  </td><td>just     </td><td>part     </td><td>take      </td><td>done    </td><td>ha          </td></tr>\n",
       "\t<tr><td>hard       </td><td>away       </td><td>last    </td><td>think   </td><td>thing    </td><td>kinda    </td><td>twelve   </td><td>first     </td><td>places  </td><td>something   </td></tr>\n",
       "\t<tr><td>look       </td><td>fun        </td><td>home    </td><td>um      </td><td>american </td><td>hhh      </td><td>chinese  </td><td>mmm       </td><td>probably</td><td>time        </td></tr>\n",
       "\t<tr><td>lots       </td><td>hahah      </td><td>years   </td><td>come    </td><td>sure     </td><td>probably </td><td>fifteen  </td><td>roof      </td><td>tsk     </td><td>live        </td></tr>\n",
       "\t<tr><td>well       </td><td>interesting</td><td>came    </td><td>south   </td><td>tsk      </td><td>pretty   </td><td>lot      </td><td>like      </td><td>totally </td><td>friends     </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 20 × 10\n",
       "\\begin{tabular}{llllllllll}\n",
       " topic1 & topic2 & topic3 & topic4 & topic5 & topic6 & topic7 & topic8 & topic9 & topic10\\\\\n",
       " <chr> & <chr> & <chr> & <chr> & <chr> & <chr> & <chr> & <chr> & <chr> & <chr>\\\\\n",
       "\\hline\n",
       "\t like        & oh          & um       & know     & like      & uh        & yeah      & yeah       & like     & really      \\\\\n",
       "\t just        & ok          & one      & well     & hh        & know      & mm        & haha       & just     & good        \\\\\n",
       "\t know        & okay        & like     & go       & um        & got       & hmm       & hahah      & hh       & little      \\\\\n",
       "\t kind        & yeah        & school   & ah       & different & come      & nice      & think      & really   & think       \\\\\n",
       "\t people      & haha        & year     & right    & kinda     & australia & haha      & hahaah     & hahaha   & bit         \\\\\n",
       "\t say         & yep         & actually & now      & cause     & always    & meet      & definitely & go       & um          \\\\\n",
       "\t things      & hahaha      & cause    & see      & lot       & work      & hoping    & right      & wanna    & can         \\\\\n",
       "\t get         & yes         & get      & mean     & sort      & never     & hm        & hi         & much     & still       \\\\\n",
       "\t stuff       & really      & time     & just     & whole     & something & hahaha    & true       & gonna    & like        \\\\\n",
       "\t think       & good        & two      & going    & well      & get       & times     & live       & thought  & moved       \\\\\n",
       "\t everything  & cool        & friends  & went     & think     & small     & looks     & long       & want     & pretty      \\\\\n",
       "\t cause       & god         & went     & states   & english   & states    & alright   & nothing    & day      & people      \\\\\n",
       "\t interesting & hh          & degree   & new      & even      & years     & edinburgh & days       & stuff    & s-          \\\\\n",
       "\t talk        & wow         & first    & probably & guess     & worked    & flights   & every      & one      & conversation\\\\\n",
       "\t everyone    & take        & back     & back     & got       & maybe     & fruit     & spent      & going    & find        \\\\\n",
       "\t way         & food        & couple   & used     & america   & just      & part      & take       & done     & ha          \\\\\n",
       "\t hard        & away        & last     & think    & thing     & kinda     & twelve    & first      & places   & something   \\\\\n",
       "\t look        & fun         & home     & um       & american  & hhh       & chinese   & mmm        & probably & time        \\\\\n",
       "\t lots        & hahah       & years    & come     & sure      & probably  & fifteen   & roof       & tsk      & live        \\\\\n",
       "\t well        & interesting & came     & south    & tsk       & pretty    & lot       & like       & totally  & friends     \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 20 × 10\n",
       "\n",
       "| topic1 &lt;chr&gt; | topic2 &lt;chr&gt; | topic3 &lt;chr&gt; | topic4 &lt;chr&gt; | topic5 &lt;chr&gt; | topic6 &lt;chr&gt; | topic7 &lt;chr&gt; | topic8 &lt;chr&gt; | topic9 &lt;chr&gt; | topic10 &lt;chr&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|\n",
       "| like        | oh          | um       | know     | like      | uh        | yeah      | yeah       | like     | really       |\n",
       "| just        | ok          | one      | well     | hh        | know      | mm        | haha       | just     | good         |\n",
       "| know        | okay        | like     | go       | um        | got       | hmm       | hahah      | hh       | little       |\n",
       "| kind        | yeah        | school   | ah       | different | come      | nice      | think      | really   | think        |\n",
       "| people      | haha        | year     | right    | kinda     | australia | haha      | hahaah     | hahaha   | bit          |\n",
       "| say         | yep         | actually | now      | cause     | always    | meet      | definitely | go       | um           |\n",
       "| things      | hahaha      | cause    | see      | lot       | work      | hoping    | right      | wanna    | can          |\n",
       "| get         | yes         | get      | mean     | sort      | never     | hm        | hi         | much     | still        |\n",
       "| stuff       | really      | time     | just     | whole     | something | hahaha    | true       | gonna    | like         |\n",
       "| think       | good        | two      | going    | well      | get       | times     | live       | thought  | moved        |\n",
       "| everything  | cool        | friends  | went     | think     | small     | looks     | long       | want     | pretty       |\n",
       "| cause       | god         | went     | states   | english   | states    | alright   | nothing    | day      | people       |\n",
       "| interesting | hh          | degree   | new      | even      | years     | edinburgh | days       | stuff    | s-           |\n",
       "| talk        | wow         | first    | probably | guess     | worked    | flights   | every      | one      | conversation |\n",
       "| everyone    | take        | back     | back     | got       | maybe     | fruit     | spent      | going    | find         |\n",
       "| way         | food        | couple   | used     | america   | just      | part      | take       | done     | ha           |\n",
       "| hard        | away        | last     | think    | thing     | kinda     | twelve    | first      | places   | something    |\n",
       "| look        | fun         | home     | um       | american  | hhh       | chinese   | mmm        | probably | time         |\n",
       "| lots        | hahah       | years    | come     | sure      | probably  | fifteen   | roof       | tsk      | live         |\n",
       "| well        | interesting | came     | south    | tsk       | pretty    | lot       | like       | totally  | friends      |\n",
       "\n"
      ],
      "text/plain": [
       "   topic1      topic2      topic3   topic4   topic5    topic6    topic7   \n",
       "1  like        oh          um       know     like      uh        yeah     \n",
       "2  just        ok          one      well     hh        know      mm       \n",
       "3  know        okay        like     go       um        got       hmm      \n",
       "4  kind        yeah        school   ah       different come      nice     \n",
       "5  people      haha        year     right    kinda     australia haha     \n",
       "6  say         yep         actually now      cause     always    meet     \n",
       "7  things      hahaha      cause    see      lot       work      hoping   \n",
       "8  get         yes         get      mean     sort      never     hm       \n",
       "9  stuff       really      time     just     whole     something hahaha   \n",
       "10 think       good        two      going    well      get       times    \n",
       "11 everything  cool        friends  went     think     small     looks    \n",
       "12 cause       god         went     states   english   states    alright  \n",
       "13 interesting hh          degree   new      even      years     edinburgh\n",
       "14 talk        wow         first    probably guess     worked    flights  \n",
       "15 everyone    take        back     back     got       maybe     fruit    \n",
       "16 way         food        couple   used     america   just      part     \n",
       "17 hard        away        last     think    thing     kinda     twelve   \n",
       "18 look        fun         home     um       american  hhh       chinese  \n",
       "19 lots        hahah       years    come     sure      probably  fifteen  \n",
       "20 well        interesting came     south    tsk       pretty    lot      \n",
       "   topic8     topic9   topic10     \n",
       "1  yeah       like     really      \n",
       "2  haha       just     good        \n",
       "3  hahah      hh       little      \n",
       "4  think      really   think       \n",
       "5  hahaah     hahaha   bit         \n",
       "6  definitely go       um          \n",
       "7  right      wanna    can         \n",
       "8  hi         much     still       \n",
       "9  true       gonna    like        \n",
       "10 live       thought  moved       \n",
       "11 long       want     pretty      \n",
       "12 nothing    day      people      \n",
       "13 days       stuff    s-          \n",
       "14 every      one      conversation\n",
       "15 spent      going    find        \n",
       "16 take       done     ha          \n",
       "17 first      places   something   \n",
       "18 mmm        probably time        \n",
       "19 roof       tsk      live        \n",
       "20 like       totally  friends     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Topics by turn\n",
    "lda_turns <- textmodel_lda(convo_turn_dfm, k=10)\n",
    "turn_results <- terms(lda_turns, n=20) |> as.data.frame()\n",
    "\n",
    "write_xlsx(turn_results, \"results/turn_topic_top_words.xlsx\")\n",
    "\n",
    "turn_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1592522a-974d-4d14-b77f-543fc83924e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 20 × 10</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>topic1</th><th scope=col>topic2</th><th scope=col>topic3</th><th scope=col>topic4</th><th scope=col>topic5</th><th scope=col>topic6</th><th scope=col>topic7</th><th scope=col>topic8</th><th scope=col>topic9</th><th scope=col>topic10</th></tr>\n",
       "\t<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>born    </td><td>haha      </td><td>worked        </td><td>sort        </td><td>ok      </td><td>yeah    </td><td>hh         </td><td>uh       </td><td>hh       </td><td>right      </td></tr>\n",
       "\t<tr><td>town    </td><td>hh        </td><td>electrician   </td><td>brisbane    </td><td>bit     </td><td>like    </td><td>um         </td><td>ok       </td><td>haha     </td><td>ok         </td></tr>\n",
       "\t<tr><td>lots    </td><td>hahaha    </td><td>ok            </td><td>found       </td><td>city    </td><td>um      </td><td>okay       </td><td>think    </td><td>okay     </td><td>hhh        </td></tr>\n",
       "\t<tr><td>also    </td><td>okay      </td><td>chuck         </td><td>non-native  </td><td>floor   </td><td>just    </td><td>different  </td><td>well     </td><td>hahaha   </td><td>americans  </td></tr>\n",
       "\t<tr><td>hour    </td><td>good      </td><td>japanese      </td><td>gosh        </td><td>guess   </td><td>oh      </td><td>mm         </td><td>states   </td><td>tsk      </td><td>mean       </td></tr>\n",
       "\t<tr><td>high    </td><td>hahah     </td><td>enough        </td><td>probably    </td><td>bk      </td><td>know    </td><td>uh         </td><td>probably </td><td>idioms   </td><td>south      </td></tr>\n",
       "\t<tr><td>alright </td><td>nice      </td><td>town          </td><td>business    </td><td>mmm     </td><td>really  </td><td>hmm        </td><td>got      </td><td>hahaah   </td><td>coast      </td></tr>\n",
       "\t<tr><td>side    </td><td>hahaah    </td><td>south         </td><td>quite       </td><td>wing    </td><td>mm      </td><td>ha         </td><td>able     </td><td>hahahaha </td><td>sure       </td></tr>\n",
       "\t<tr><td>group   </td><td>gonna     </td><td>grow          </td><td>early       </td><td>small   </td><td>go      </td><td>sort       </td><td>small    </td><td>speakers </td><td>nothing    </td></tr>\n",
       "\t<tr><td>sorry   </td><td>yes       </td><td>fruit         </td><td>try         </td><td>guys    </td><td>cause   </td><td>thing      </td><td>come     </td><td>hahah    </td><td>two        </td></tr>\n",
       "\t<tr><td>months  </td><td>cool      </td><td>apprenticeship</td><td>completely  </td><td>wow     </td><td>get     </td><td>got        </td><td>sort     </td><td>end      </td><td>east       </td></tr>\n",
       "\t<tr><td>four    </td><td>tsk       </td><td>help          </td><td>dissertation</td><td>flats   </td><td>one     </td><td>interesting</td><td>degree   </td><td>nudity   </td><td>programme  </td></tr>\n",
       "\t<tr><td>real    </td><td>fun       </td><td>name          </td><td>maroochydore</td><td>together</td><td>well    </td><td>haha       </td><td>little   </td><td>funny    </td><td>australians</td></tr>\n",
       "\t<tr><td>well    </td><td>food      </td><td>closed        </td><td>afternoon   </td><td>good    </td><td>good    </td><td>spanish    </td><td>maybe    </td><td>totally  </td><td>old        </td></tr>\n",
       "\t<tr><td>true    </td><td>wanna     </td><td>drive         </td><td>uk          </td><td>funny   </td><td>people  </td><td>accent     </td><td>might    </td><td>process  </td><td>hoping     </td></tr>\n",
       "\t<tr><td>day     </td><td>chilli    </td><td>somebody      </td><td>hey         </td><td>coming  </td><td>think   </td><td>kinda      </td><td>work     </td><td>amsterdam</td><td>part       </td></tr>\n",
       "\t<tr><td>students</td><td>exchange  </td><td>head          </td><td>unless      </td><td>bus     </td><td>kinda   </td><td>english    </td><td>plant    </td><td>sorry    </td><td>scottish   </td></tr>\n",
       "\t<tr><td>wales   </td><td>three     </td><td>short         </td><td>recording   </td><td>possums </td><td>kind    </td><td>s-         </td><td>something</td><td>first    </td><td>europe     </td></tr>\n",
       "\t<tr><td>doctor  </td><td>going     </td><td>farming       </td><td>needed      </td><td>roof    </td><td>actually</td><td>degree     </td><td>school   </td><td>english  </td><td>street     </td></tr>\n",
       "\t<tr><td>teacher </td><td>university</td><td>ohio          </td><td>ooh         </td><td>watched </td><td>back    </td><td>something  </td><td>back     </td><td>anything </td><td>s'pose     </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 20 × 10\n",
       "\\begin{tabular}{llllllllll}\n",
       " topic1 & topic2 & topic3 & topic4 & topic5 & topic6 & topic7 & topic8 & topic9 & topic10\\\\\n",
       " <chr> & <chr> & <chr> & <chr> & <chr> & <chr> & <chr> & <chr> & <chr> & <chr>\\\\\n",
       "\\hline\n",
       "\t born     & haha       & worked         & sort         & ok       & yeah     & hh          & uh        & hh        & right      \\\\\n",
       "\t town     & hh         & electrician    & brisbane     & bit      & like     & um          & ok        & haha      & ok         \\\\\n",
       "\t lots     & hahaha     & ok             & found        & city     & um       & okay        & think     & okay      & hhh        \\\\\n",
       "\t also     & okay       & chuck          & non-native   & floor    & just     & different   & well      & hahaha    & americans  \\\\\n",
       "\t hour     & good       & japanese       & gosh         & guess    & oh       & mm          & states    & tsk       & mean       \\\\\n",
       "\t high     & hahah      & enough         & probably     & bk       & know     & uh          & probably  & idioms    & south      \\\\\n",
       "\t alright  & nice       & town           & business     & mmm      & really   & hmm         & got       & hahaah    & coast      \\\\\n",
       "\t side     & hahaah     & south          & quite        & wing     & mm       & ha          & able      & hahahaha  & sure       \\\\\n",
       "\t group    & gonna      & grow           & early        & small    & go       & sort        & small     & speakers  & nothing    \\\\\n",
       "\t sorry    & yes        & fruit          & try          & guys     & cause    & thing       & come      & hahah     & two        \\\\\n",
       "\t months   & cool       & apprenticeship & completely   & wow      & get      & got         & sort      & end       & east       \\\\\n",
       "\t four     & tsk        & help           & dissertation & flats    & one      & interesting & degree    & nudity    & programme  \\\\\n",
       "\t real     & fun        & name           & maroochydore & together & well     & haha        & little    & funny     & australians\\\\\n",
       "\t well     & food       & closed         & afternoon    & good     & good     & spanish     & maybe     & totally   & old        \\\\\n",
       "\t true     & wanna      & drive          & uk           & funny    & people   & accent      & might     & process   & hoping     \\\\\n",
       "\t day      & chilli     & somebody       & hey          & coming   & think    & kinda       & work      & amsterdam & part       \\\\\n",
       "\t students & exchange   & head           & unless       & bus      & kinda    & english     & plant     & sorry     & scottish   \\\\\n",
       "\t wales    & three      & short          & recording    & possums  & kind     & s-          & something & first     & europe     \\\\\n",
       "\t doctor   & going      & farming        & needed       & roof     & actually & degree      & school    & english   & street     \\\\\n",
       "\t teacher  & university & ohio           & ooh          & watched  & back     & something   & back      & anything  & s'pose     \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 20 × 10\n",
       "\n",
       "| topic1 &lt;chr&gt; | topic2 &lt;chr&gt; | topic3 &lt;chr&gt; | topic4 &lt;chr&gt; | topic5 &lt;chr&gt; | topic6 &lt;chr&gt; | topic7 &lt;chr&gt; | topic8 &lt;chr&gt; | topic9 &lt;chr&gt; | topic10 &lt;chr&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|\n",
       "| born     | haha       | worked         | sort         | ok       | yeah     | hh          | uh        | hh        | right       |\n",
       "| town     | hh         | electrician    | brisbane     | bit      | like     | um          | ok        | haha      | ok          |\n",
       "| lots     | hahaha     | ok             | found        | city     | um       | okay        | think     | okay      | hhh         |\n",
       "| also     | okay       | chuck          | non-native   | floor    | just     | different   | well      | hahaha    | americans   |\n",
       "| hour     | good       | japanese       | gosh         | guess    | oh       | mm          | states    | tsk       | mean        |\n",
       "| high     | hahah      | enough         | probably     | bk       | know     | uh          | probably  | idioms    | south       |\n",
       "| alright  | nice       | town           | business     | mmm      | really   | hmm         | got       | hahaah    | coast       |\n",
       "| side     | hahaah     | south          | quite        | wing     | mm       | ha          | able      | hahahaha  | sure        |\n",
       "| group    | gonna      | grow           | early        | small    | go       | sort        | small     | speakers  | nothing     |\n",
       "| sorry    | yes        | fruit          | try          | guys     | cause    | thing       | come      | hahah     | two         |\n",
       "| months   | cool       | apprenticeship | completely   | wow      | get      | got         | sort      | end       | east        |\n",
       "| four     | tsk        | help           | dissertation | flats    | one      | interesting | degree    | nudity    | programme   |\n",
       "| real     | fun        | name           | maroochydore | together | well     | haha        | little    | funny     | australians |\n",
       "| well     | food       | closed         | afternoon    | good     | good     | spanish     | maybe     | totally   | old         |\n",
       "| true     | wanna      | drive          | uk           | funny    | people   | accent      | might     | process   | hoping      |\n",
       "| day      | chilli     | somebody       | hey          | coming   | think    | kinda       | work      | amsterdam | part        |\n",
       "| students | exchange   | head           | unless       | bus      | kinda    | english     | plant     | sorry     | scottish    |\n",
       "| wales    | three      | short          | recording    | possums  | kind     | s-          | something | first     | europe      |\n",
       "| doctor   | going      | farming        | needed       | roof     | actually | degree      | school    | english   | street      |\n",
       "| teacher  | university | ohio           | ooh          | watched  | back     | something   | back      | anything  | s'pose      |\n",
       "\n"
      ],
      "text/plain": [
       "   topic1   topic2     topic3         topic4       topic5   topic6  \n",
       "1  born     haha       worked         sort         ok       yeah    \n",
       "2  town     hh         electrician    brisbane     bit      like    \n",
       "3  lots     hahaha     ok             found        city     um      \n",
       "4  also     okay       chuck          non-native   floor    just    \n",
       "5  hour     good       japanese       gosh         guess    oh      \n",
       "6  high     hahah      enough         probably     bk       know    \n",
       "7  alright  nice       town           business     mmm      really  \n",
       "8  side     hahaah     south          quite        wing     mm      \n",
       "9  group    gonna      grow           early        small    go      \n",
       "10 sorry    yes        fruit          try          guys     cause   \n",
       "11 months   cool       apprenticeship completely   wow      get     \n",
       "12 four     tsk        help           dissertation flats    one     \n",
       "13 real     fun        name           maroochydore together well    \n",
       "14 well     food       closed         afternoon    good     good    \n",
       "15 true     wanna      drive          uk           funny    people  \n",
       "16 day      chilli     somebody       hey          coming   think   \n",
       "17 students exchange   head           unless       bus      kinda   \n",
       "18 wales    three      short          recording    possums  kind    \n",
       "19 doctor   going      farming        needed       roof     actually\n",
       "20 teacher  university ohio           ooh          watched  back    \n",
       "   topic7      topic8    topic9    topic10    \n",
       "1  hh          uh        hh        right      \n",
       "2  um          ok        haha      ok         \n",
       "3  okay        think     okay      hhh        \n",
       "4  different   well      hahaha    americans  \n",
       "5  mm          states    tsk       mean       \n",
       "6  uh          probably  idioms    south      \n",
       "7  hmm         got       hahaah    coast      \n",
       "8  ha          able      hahahaha  sure       \n",
       "9  sort        small     speakers  nothing    \n",
       "10 thing       come      hahah     two        \n",
       "11 got         sort      end       east       \n",
       "12 interesting degree    nudity    programme  \n",
       "13 haha        little    funny     australians\n",
       "14 spanish     maybe     totally   old        \n",
       "15 accent      might     process   hoping     \n",
       "16 kinda       work      amsterdam part       \n",
       "17 english     plant     sorry     scottish   \n",
       "18 s-          something first     europe     \n",
       "19 degree      school    english   street     \n",
       "20 something   back      anything  s'pose     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Topics on conversations as documents\n",
    "lda_convo <- textmodel_lda(convo_dfm, k=10)\n",
    "convo_results <- terms(lda_convo, n=20) |> as.data.frame()\n",
    "\n",
    "write_xlsx(convo_results, \"results/convo_topic_top_words.xlsx\")\n",
    "\n",
    "convo_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
