{"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"codemirror_mode":"r","file_extension":".r","mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"4.3.3"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"library(quanteda)\nlibrary(quanteda.textstats)\nlibrary(writexl)\nlibrary(seededlda)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(pheatmap)\nlibrary(tibble)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[],"id":"45d2a408-d06d-4d67-95c0-72cfd86dbbad"},{"cell_type":"code","source":"# Load data and list column names\nconvos <- read.csv('data/all-conversations.csv')\n\n# Focus our analysis on only the turns purely in the MAIN section - note that there are a small number of turns that overlap \n# between main and pre/post as different conversation groups were overlapping at the time of data collection.\nconvos <- convos[convos$section == 'MAIN',]\n\n# Generate a compact conversation ID from the file_id for compact labels on plots\nconvos$conversation_id <- regmatches(convos$X_file_id, regexpr(\"AmAus[0-9]+\", convos$X_file_id))\n\n# generate a doc_id for quanteda - this would ideally be a primary key instead.\nconvos$doc_id <- seq_along(convos$text)\n\nnrow(convos)\nnames(convos)","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[],"id":"00be7e80-900b-4694-9d28-8ce945939c86"},{"cell_type":"code","source":"# Create a corpus from the dataframe we loaded, use the text field as the text\nconvo_corpus <- corpus(convos, text_field=\"text\", docid_field = \"doc_id\")\n\n# Tokenise the full dataset - use standard tokeniser, but remove punctuation and lowercase all tokens\nconvo_full_tokens <- tokens_tolower(\n    tokens(convo_corpus, remove_punct=TRUE)\n)\n\nprint(paste(\"Total tokens\", sum(ntoken(dfm(convo_full_tokens)))))\nprint(paste(\"Total types\", length(types(convo_full_tokens))))\n\nfrequent_tokens <- c(\n    # This is based on the top 100 most frequent tokens after\n    # removing the standard English stop list, preserving some common\n    # nouns like \"Brisbane\", \"Australia\", and \"friends\"\n    \"yeah\",\n    \"like\",\n    \"hh\",\n    \"um\",\n    \"know\",\n    \"just\",\n    \"oh\",\n    \"mm\",\n    \"really\",\n    \"haha\",\n    \"think\",\n    \"well\",\n    \"uh\",\n    \"cause\",\n    \"go\",\n    \"ok\",\n    \"get\",\n    \"people\",\n    \"one\",\n    \"right\",\n    \"hahaha\",\n    \"tsk\",\n    \"ah\",\n    \"got\",\n    \"good\",\n    \"hmm\",\n    \"kind\",\n    \"mean\",\n    \"can\",\n    \"kinda\",\n    \"lot\",\n    \"actually\",\n    \"okay\",\n    \"pretty\",\n    \"different\",\n    \"cool\",\n    \"little\",\n    \"much\",\n    \"thing\",\n    \"now\",\n    \"year\",\n    \"stuff\",\n    \"something\",\n    \"yep\",\n    \"went\",\n    \"back\",\n    \"years\",\n    \"two\",\n    \"bit\",\n    \"sort\",\n    \"time\",\n    \"see\",\n    \"even\",\n    \"say\",\n    \"going\",\n    \"nice\",\n    \"things\",\n    \"come\",\n    \"gonna\",\n    \"wanna\",\n    \"us\",\n    \"interesting\",\n    \"guess\",\n    \"work\",\n    \"states\",\n    \"still\",\n    \"probably\",\n    \"big\",\n    \"new\",\n    \"yes\",\n    \"way\",\n    \"first\",\n    \"never\",\n    \"three\",\n    \"sure\",\n    \"hhh\",\n    \"around\",\n    \"always\",\n    \"everything\",\n    \"i-\",\n    \"said\",\n    \"wow\",\n    \"came\",\n    \"ha\",\n    \"mhm\",\n    \"alright\",\n    \"hahahaha\",\n    \"day\",\n    \"take\",\n    \"want\",\n    \"used\",\n    \"long\",\n    \"y-\"\n)\ntranscription_marks <- c(\n    \"Â°\",\n    \"sniffs\",\n    \"hh\",\n    \"hhh\"\n)\ncustom_stopwords <- c(\n    frequent_tokens,\n    transcription_marks,\n    stopwords(\"english\")\n)\n\n# Tokenise the corpus - we use the Quanteda default tokenisation and remove the standard list of English stopwords\n# Note that the standard English list assumes written not spoken material so we will have to take a closer look at this.\nconvo_tokens <- convo_full_tokens |> \n# Remove our custom stopwords\ntokens_remove(custom_stopwords, min_nchar=2) |>\n# Remove partial tokens using a match on trailing hyphen\ntokens_remove(\"*-\", valuetype=\"glob\")\n\n# Create a document-feature matrix, (also known as document term, or term-document matrix).\n# Note that dfm is the quanteda standard nomenclature so I'll use it throughout.\n# More specifically this is a Turn-Token matrix, as the 'documents' are single turns by a speaker.\n# For computational reasons we will trim any token that occurs only once to reduce the vocabulary size.\nconvo_turn_dfm <- dfm_trim(dfm(convo_tokens), min_termfreq = 2)\n","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[],"id":"5d220c59-2e47-4333-8d9f-a37c7343c485"},{"cell_type":"markdown","source":"# Granularity\n\nWe can count things at different levels of granularity to start to address topic. A word that is used once in every conversation is different from a word used 30 times in a single conversation.\n\nSome examples:\n\n- we can count tokens, regardless of where they occur\n- we can count turns including a token\n- we can count conversations including a token\n- we can count by speaker in a conversation\n\n","metadata":{"tags":[]},"id":"ecd56ed9-4e24-4928-8550-9056c0ba88a8"},{"cell_type":"code","source":"# Let's start with tokens and turns:\nfrequencies <- textstat_frequency(convo_turn_dfm)\nnames(frequencies)\n\n# the feature is the token, the frequency is the token count, and docfreq is the turn count\nwrite_xlsx(frequencies, \"results/token_turn_counts.xlsx\")","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[],"id":"852afca2-80e1-466d-a558-a56df8f8f95c"},{"cell_type":"code","source":"# We'll group the dfm together by the file (conversation) to count tokens and conversations\nconvo_dfm <- dfm_group(convo_turn_dfm, groups=docvars(convo_corpus, 'conversation_id'))\nconversation_frequencies <- textstat_frequency(convo_dfm)\n\nwrite_xlsx(conversation_frequencies, \"results/token_conversation_counts.xlsx\")","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[],"id":"878ac6cb-487d-44e3-af9f-d2b71f370d44"},{"cell_type":"markdown","source":"# Topic modelling and what's in a document\n\nTopic modelling algorithms like LDA work with \"documents\" containing \"bags of words\" (counts of how often each word occurs, regardless of their order). For analysing\nconversations this means we have to make an analytical choice about what is the unit of a document.\n\nWe'll explore two extreme ends of this granularity question:\n\n1. Each turn in the conversation is a document.\n2. Each conversation is a document, including all the turns concatenated together.\n\nTreating each turn as a document should hopefully yield focused and precise topics, but because turns are generally short and LDA like algorithms are thought to perform better with longer documents results may not be great. Treating each conversation as a document might give more traction for an algorithm as there is more opportunity to investigate word co-occurence, at the cost of potentially mixing things too much. As we would expect the topic of conversation to shift over time in a single conversation it may not be appropriate to have the first and last turns in a conversation mixed together.\n","metadata":{},"id":"f86bb87e-33d0-472d-b2e0-cedada677a83"},{"cell_type":"code","source":"# Before we do the actual calculation, let's make a helper function to visualise the results a bit\n# more nicely and with more information than the default functionality of the textmodels package.\ncreate_topic_table <- function(lda_model, n_terms=20) {\n    # Take an LDA model and create a more nicely formatted table.\n    # All of the prevalence measures are converted to percentages and rounded for display.\n    # Output format: topic_number, topic_prevalence, topic_words (prevalence),\n    topic_prevalence <- colMeans(lda_model$theta) |> \n        as.data.frame() |>\n        rownames_to_column(var=\"topic\")\n    \n    colnames(topic_prevalence) <- c(\"topic\", \"prevalence\")\n        \n    \n    # Now extract the top words and weights for each topic->term probability\n    # Unfortunately this duplicates the terms function which doesn't return\n    # the weights.\n    topic_terms <- lda_model$phi |> \n        as.data.frame() |>\n        # Need to convert the matrix rownames on the dataframe to a column for tidyverse\n        rownames_to_column(var=\"topic_id\") |>\n        # Pivot so we have a long table to make grouping easy\n        pivot_longer(!topic_id, names_to=\"feature\", values_to=\"weight\") |>\n        rename(topic=topic_id) |>\n        # Create the form <token> {<weight} for representing the table \n        mutate(weight_rep = paste0(feature, \" (\", round(weight * 100, 1), \")\")) |>\n        # Group by topic and select the top weighted terms, sort in descending order\n        group_by(topic) |>\n        top_n(weight, n=n_terms) |>\n        arrange(topic, desc(weight)) |>\n        # Finally collapse all the features into a single row per topic\n        summarise(topic_features = paste(weight_rep, sep=\", \", collapse=\" \"))\n        \n    combined <- topic_terms |>\n        inner_join(topic_prevalence, by=join_by(topic)) |>\n        arrange(desc(prevalence)) |>\n        select(topic, prevalence, topic_features) |>\n        mutate(prevalence = round(prevalence * 100, 2))\n    \n    return(combined)\n}","metadata":{"trusted":true,"tags":[]},"execution_count":null,"outputs":[],"id":"a707018f-eb23-4a5b-9181-8f41568d5afe"},{"cell_type":"markdown","source":"# Set core topic model and display parameters\n\nThe following parameters are one of the core parameters to set in most topic model algorithms: the fixed number of topics to learn (sometimes labelled k), and the number of top terms from each topic we will represent in our output table.","metadata":{},"id":"4057ca5d-0f37-4376-b870-58c451d08ae7"},{"cell_type":"code","source":"number_of_topics <- 20\nprint_top_terms <- 10","metadata":{"trusted":true,"tags":[]},"execution_count":null,"outputs":[],"id":"bda25bc1-c658-4029-b99d-0d0afd54454e"},{"cell_type":"code","source":"# Topics by turn\nset.seed(192038102)\nlda_turns <- textmodel_lda(convo_turn_dfm, k=number_of_topics)\nturn_results <- terms(lda_turns, n=print_top_terms) |> as.data.frame()\n\nwrite_xlsx(turn_results, \"results/turn_topic_top_words.xlsx\")\n\nturn_lda_table <- create_topic_table(lda_turns, n_terms = print_top_terms)\nwrite_xlsx(turn_lda_table, \"results/turn_topic_table.xlsx\")\n\nturn_results","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[],"id":"a452475a-32c2-4434-bdbe-cdf2978cca16"},{"cell_type":"code","source":"# Topics on conversations as documents\nset.seed(50193853)\nlda_convo <- textmodel_lda(convo_dfm, k=number_of_topics)\nconvo_results <- terms(lda_convo, n=print_top_terms) |> as.data.frame()\n\nwrite_xlsx(convo_results, \"results/convo_topic_top_words.xlsx\")\n\nconvo_lda_table <- create_topic_table(lda_convo, n_terms = print_top_terms)\nwrite_xlsx(convo_lda_table, \"results/convo_topic_table.xlsx\")\n\nconvo_results","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[],"id":"1592522a-974d-4d14-b77f-543fc83924e0"},{"cell_type":"code","source":"# Heatmap of topics x conversations\n\n# First of all - let's rename the topics to use the first few words of each topic as an indicator\ntopic_convo_weights <- t(lda_convo$theta)\n\ntopic_labels <- terms(lda_convo, n=3) |> \n    as.data.frame() |>\n    pivot_longer(everything(), names_to=\"topic\") |>\n    group_by(topic) |>\n    summarise(features=paste0(value, collapse=\" \"))\n    \nrownames(topic_convo_weights) <- topic_labels$features[match(rownames(topic_convo_weights), topic_labels$topic)]\n\npheatmap(\n    topic_convo_weights,\n    cluster_rows = FALSE,\n    cluster_cols = FALSE,\n    color = colorRampPalette(c(\"white\", \"red\"))(20),\n    cellwidth = 16,\n    cellheight = 16,\n    angle_col=\"90\",\n    width = 10,\n    height = 6,\n    file=\"results/convo_topic_heatmap.pdf\"\n)\n","metadata":{"trusted":true,"tags":[]},"execution_count":null,"outputs":[],"id":"271d6b11-4780-4066-96f4-b04c3b045c41"},{"cell_type":"code","source":"options(repr.matrix.max.rows=500)\n        \n# Keyword in (conversational) context - to do this we need to retrieve the turns containing the keyword, \n# and show it in context of surrounding turns.\n\n# Show this number of turns either side of each match\nturn_window_size = 3\npattern = 'rock'\n\n# This is annoyingly fiddly - grab the matching turns and extract the original doc_id/sequence in our corpus df.\n# I feel like there should be a better way to do this?\n# We also need to take the unique docids and pre-compute window offsets for displaying the surrounding context. \nmatching_turns <- data.frame(matchid = as.integer(unique(index(convo_tokens, pattern = pattern)$docname))) |>\n    mutate(window_start = matchid - turn_window_size, window_end = matchid + turn_window_size) |>\n    select(matchid, window_start, window_end)\n\nturns_with_context <- convos |> inner_join(matching_turns, join_by(between(doc_id, window_start, window_end))) |>\n    arrange(matchid, doc_id) |>\n    select(name, text, timeCode, conversation_id, matchid, doc_id)  \n\nfilename <- paste(\"results/snippets_\", pattern, \".xlsx\", sep = \"\", collapse = NULL)\n\nprint(paste(\"wrote out \", nrow(matching_turns), \" matches to: \", filename, sep=\"\", collapse=NULL))\n\nwrite_xlsx(turns_with_context, filename)\n\nturns_with_context\n","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[],"id":"7a52738f-37e2-42b5-8542-17b7b0e5f5c7"}]}